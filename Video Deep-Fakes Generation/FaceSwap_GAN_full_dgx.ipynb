{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras\n",
    "!pip install scikit-image\n",
    "!pip install moviepy\n",
    "!pip install torch\n",
    "!pip install f2format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8863,
     "status": "ok",
     "timestamp": 1570073073276,
     "user": {
      "displayName": "Deep Learning",
      "photoUrl": "",
      "userId": "02790113473800056642"
     },
     "user_tz": -330
    },
    "id": "Y2sGAim9V_aI",
    "outputId": "77634aed-aadd-41b1-b212-96232da05f4b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import time\n",
    "from pathlib import PurePath, Path\n",
    "from glob import glob\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras import regularizers\n",
    "from moviepy.editor import VideoFileClip\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kVCk8ElNWFlV"
   },
   "outputs": [],
   "source": [
    "# Only do this once then comment it out\n",
    "# !git clone https://github.com/shaoanlu/faceswap-GAN.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e8cHxhSBhahl"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "os.chdir('faceswap-GAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only required if Python version is < 3.6, as f-strings were introduced in 3.6\n",
    "!f2format networks\n",
    "!f2format converter\n",
    "!f2format data_loader\n",
    "!f2format detector\n",
    "!f2format utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQE-bknXgWxF"
   },
   "outputs": [],
   "source": [
    "# Adds import statement to networks/nn_blocks.py\n",
    "# Only do this once then comment it out\n",
    "\"\"\"temp_str = ''\n",
    "with open('networks/nn_blocks.py','r') as f:\n",
    "  temp_str = f.read()\n",
    "\n",
    "temp_str = 'import keras.backend as K\\nfrom keras import regularizers\\n'.join(temp_str.split('import keras.backend as K\\n'))\n",
    "\n",
    "with open('networks/nn_blocks.py','w') as f:\n",
    "  f.write(temp_str)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UEH-FTvIaupl"
   },
   "outputs": [],
   "source": [
    "# Only do this once then comment it out\n",
    "!git clone https://github.com/1adrianb/face-alignment.git\n",
    "shutil.move('face-alignment/face_alignment','face_alignment')\n",
    "shutil.rmtree('face-alignment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7gTqKWJxvUAl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Only do this once then comment it out\n",
    "!git clone https://github.com/rcmalli/keras-vggface.git\n",
    "shutil.move('keras-vggface/keras_vggface','keras_vggface')\n",
    "shutil.rmtree('keras-vggface')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6mQvmF0yWe4w"
   },
   "source": [
    "## Choose Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oxm6RbZMngx6"
   },
   "outputs": [],
   "source": [
    "# youtube-dl if links available\n",
    "# !pip install youtube-dl\n",
    "# !youtube-dl -f best -o <filenameA> <linkA>\n",
    "# !youtube-dl -f best -o <filenameB> <linkB>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "G4ycOKKJWeok"
   },
   "outputs": [],
   "source": [
    "vid1_name = 'ankit.mp4'\n",
    "vid2_name = 'vaibhav.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ma4j9D9vqjEv"
   },
   "source": [
    "# Face detection for video\n",
    "Images of detected faces have format `frameXfaceY.jpg`, where `X` represents the Xth frame and `Y` the Yth face in Xth frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cFBtIdgIqjEy"
   },
   "outputs": [],
   "source": [
    "from umeyama import umeyama\n",
    "import mtcnn_detect_face\n",
    "\n",
    "def create_mtcnn(sess, model_path):\n",
    "    if not model_path:\n",
    "        model_path,_ = os.path.split(os.path.realpath(__file__))\n",
    "\n",
    "    with tf.variable_scope('pnet2'):\n",
    "        data = tf.placeholder(tf.float32, (None,None,None,3), 'input')\n",
    "        pnet = mtcnn_detect_face.PNet({'data':data})\n",
    "        pnet.load(os.path.join(model_path, 'det1.npy'), sess)\n",
    "    with tf.variable_scope('rnet2'):\n",
    "        data = tf.placeholder(tf.float32, (None,24,24,3), 'input')\n",
    "        rnet = mtcnn_detect_face.RNet({'data':data})\n",
    "        rnet.load(os.path.join(model_path, 'det2.npy'), sess)\n",
    "    with tf.variable_scope('onet2'):\n",
    "        data = tf.placeholder(tf.float32, (None,48,48,3), 'input')\n",
    "        onet = mtcnn_detect_face.ONet({'data':data})\n",
    "        onet.load(os.path.join(model_path, 'det3.npy'), sess)\n",
    "    return pnet, rnet, onet\n",
    "\n",
    "WEIGHTS_PATH = \"./mtcnn_weights/\"\n",
    "\n",
    "sess = K.get_session()\n",
    "with sess.as_default():\n",
    "    global pnet, rnet, onet \n",
    "    pnet, rnet, onet = create_mtcnn(sess, WEIGHTS_PATH)\n",
    "\n",
    "global pnet, rnet, onet\n",
    "    \n",
    "pnet = K.function([pnet.layers['data']],[pnet.layers['conv4-2'], pnet.layers['prob1']])\n",
    "rnet = K.function([rnet.layers['data']],[rnet.layers['conv5-2'], rnet.layers['prob1']])\n",
    "onet = K.function([onet.layers['data']],[onet.layers['conv6-2'], onet.layers['conv6-3'], onet.layers['prob1']])\n",
    "\n",
    "Path(\"aligned_faces\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_src_landmarks(x0, x1, y0, y1, pnts):\n",
    "    \"\"\"\n",
    "    x0, x1, y0, y1: (smoothed) bbox coord.\n",
    "    pnts: landmarks predicted by MTCNN\n",
    "    \"\"\"    \n",
    "    src_landmarks = [(int(pnts[i+5][0]-x0), \n",
    "                      int(pnts[i][0]-y0)) for i in range(5)]\n",
    "    return src_landmarks\n",
    "\n",
    "def get_tar_landmarks(img):\n",
    "    \"\"\"    \n",
    "    img: detected face image\n",
    "    \"\"\"         \n",
    "    ratio_landmarks = [\n",
    "        (0.31339227236234224, 0.3259269274198092),\n",
    "        (0.31075140146108776, 0.7228453709528997),\n",
    "        (0.5523683107816256, 0.5187296867370605),\n",
    "        (0.7752419985257663, 0.37262483743520886),\n",
    "        (0.7759613623985877, 0.6772957581740159)\n",
    "        ]   \n",
    "        \n",
    "    img_size = img.shape\n",
    "    tar_landmarks = [(int(xy[0]*img_size[0]), \n",
    "                      int(xy[1]*img_size[1])) for xy in ratio_landmarks]\n",
    "    return tar_landmarks\n",
    "\n",
    "def landmarks_match_mtcnn(src_im, src_landmarks, tar_landmarks): \n",
    "    \"\"\"\n",
    "    umeyama(src, dst, estimate_scale)\n",
    "    landmarks coord. for umeyama should be (width, height) or (y, x)\n",
    "    \"\"\"\n",
    "    src_size = src_im.shape\n",
    "    src_tmp = [(int(xy[1]), int(xy[0])) for xy in src_landmarks]\n",
    "    tar_tmp = [(int(xy[1]), int(xy[0])) for xy in tar_landmarks]\n",
    "    M = umeyama(np.array(src_tmp), np.array(tar_tmp), True)[0:2]\n",
    "    result = cv2.warpAffine(src_im, M, (src_size[1], src_size[0]), borderMode=cv2.BORDER_REPLICATE) \n",
    "    return result\n",
    "\n",
    "def process_mtcnn_bbox(bboxes, im_shape):\n",
    "    \"\"\"\n",
    "    output bbox coordinate of MTCNN is (y0, x0, y1, x1)\n",
    "    Here we process the bbox coord. to a square bbox with ordering (x0, y1, x1, y0)\n",
    "    \"\"\"\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        y0, x0, y1, x1 = bboxes[i,0:4]\n",
    "        w, h = int(y1 - y0), int(x1 - x0)\n",
    "        length = (w + h)/2\n",
    "        center = (int((x1+x0)/2),int((y1+y0)/2))\n",
    "        new_x0 = np.max([0, (center[0]-length//2)])#.astype(np.int32)\n",
    "        new_x1 = np.min([im_shape[0], (center[0]+length//2)])#.astype(np.int32)\n",
    "        new_y0 = np.max([0, (center[1]-length//2)])#.astype(np.int32)\n",
    "        new_y1 = np.min([im_shape[1], (center[1]+length//2)])#.astype(np.int32)\n",
    "        bboxes[i,0:4] = new_x0, new_y1, new_x1, new_y0\n",
    "    return bboxes\n",
    "\n",
    "def process_video(input_img): \n",
    "    global frames, save_interval\n",
    "    global pnet, rnet, onet\n",
    "    minsize = 30 # minimum size of face\n",
    "    detec_threshold = 0.7\n",
    "    threshold = [0.6, 0.7, detec_threshold]  # three steps's threshold\n",
    "    factor = 0.709 # scale factor   \n",
    "    \n",
    "    frames += 1    \n",
    "    if frames % save_interval == 0:\n",
    "        faces, pnts = mtcnn_detect_face.detect_face(\n",
    "            input_img, minsize, pnet, rnet, onet, threshold, factor)\n",
    "        faces = process_mtcnn_bbox(faces, input_img.shape)\n",
    "        \n",
    "        for idx, (x0, y1, x1, y0, conf_score) in enumerate(faces):\n",
    "            det_face_im = input_img[int(x0):int(x1),int(y0):int(y1),:]\n",
    "\n",
    "            # get src/tar landmarks\n",
    "            src_landmarks = get_src_landmarks(x0, x1, y0, y1, pnts)\n",
    "            tar_landmarks = get_tar_landmarks(det_face_im)\n",
    "\n",
    "            # align detected face\n",
    "            aligned_det_face_im = landmarks_match_mtcnn(\n",
    "                det_face_im, src_landmarks, tar_landmarks)\n",
    "\n",
    "            fname = \"./aligned_faces/frame\"+str(frames)+\"face\"+str(idx)+\".jpg\"\n",
    "            plt.imsave(fname, aligned_det_face_im, format=\"jpg\")\n",
    "        \n",
    "    return np.zeros((3,3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PhM8pvWvaE1q"
   },
   "outputs": [],
   "source": [
    "# Number of images of each face\n",
    "num_images = 1000\n",
    "# Create folders for Face A and Face B\n",
    "Path(\"faceA\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"faceB\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rd8GUsKyX5Z2"
   },
   "outputs": [],
   "source": [
    "# Extract faces from A\n",
    "\n",
    "global frames\n",
    "frames = 0\n",
    "\n",
    "# configuration\n",
    "fn_input_video = vid1_name\n",
    "output = vid1_name.split('.')[0] + '_faces.mp4'\n",
    "\n",
    "clip1 = VideoFileClip(fn_input_video)\n",
    "save_interval = clip1.duration*clip1.fps//num_images # perform face detection every {save_interval} frames\n",
    "clip = clip1.fl_image(process_video) # .subclip(0,3) #NOTE: this function expects color images!!\n",
    "clip.write_videofile(output, audio=False)\n",
    "clip1.reader.close()\n",
    "\n",
    "face_letter = 'A'\n",
    "\n",
    "faces_list = os.listdir('aligned_faces')\n",
    "if os.listdir('face' + face_letter):\n",
    "    raise Exception('face' + face_letter + ' contains faces. Please empty it before using this command.')\n",
    "for face in faces_list:\n",
    "    os.rename('aligned_faces/'+face,'face'+face_letter+'/'+face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zi2xjcX9aKH9"
   },
   "outputs": [],
   "source": [
    "# Extract faces from B\n",
    "\n",
    "global frames\n",
    "frames = 0\n",
    "\n",
    "# configuration\n",
    "fn_input_video = vid2_name\n",
    "output = vid2_name.split('.')[0] + '_faces.mp4'\n",
    "\n",
    "clip1 = VideoFileClip(fn_input_video)\n",
    "save_interval = clip1.duration*clip1.fps//num_images # perform face detection every {save_interval} frames\n",
    "clip = clip1.fl_image(process_video) # .subclip(0,3) #NOTE: this function expects color images!!\n",
    "clip.write_videofile(output, audio=False)\n",
    "clip1.reader.close()\n",
    "\n",
    "face_letter = 'B'\n",
    "\n",
    "faces_list = os.listdir('aligned_faces')\n",
    "if os.listdir('face' + face_letter):\n",
    "    raise Exception('face' + face_letter + ' contains faces. Please empty it before using this command.')\n",
    "for face in faces_list:\n",
    "    os.rename('aligned_faces/'+face,'face'+face_letter+'/'+face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "McqNHExgvAhU"
   },
   "outputs": [],
   "source": [
    "# Zip face folders\n",
    "!zip -r faceA.zip faceA\n",
    "!zip -r faceB.zip faceB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete face folders\n",
    "shutil.rmtree('faceA')\n",
    "shutil.rmtree('faceB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the zips, manually clean the folders by removing blurry faces or non-faces, then upload them back on the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3BdWiYl1vt_F"
   },
   "outputs": [],
   "source": [
    "# Unzip uploaded zips\n",
    "!unzip faceA.zip\n",
    "!unzip faceB.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1Agj1WDv2Gx"
   },
   "source": [
    "# Preparation of Binary Masks\n",
    "\n",
    "Creating high quality binary masks from face data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "giUCMGn1vBc1"
   },
   "outputs": [],
   "source": [
    "import face_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B7_WdnD8vBav"
   },
   "outputs": [],
   "source": [
    "dir_faceA = \"./faceA\"\n",
    "dir_faceB = \"./faceB\"\n",
    "dir_bm_faceA_eyes = \"./binary_masks/faceA_eyes\"\n",
    "dir_bm_faceB_eyes = \"./binary_masks/faceB_eyes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsx3SohVvBYL"
   },
   "outputs": [],
   "source": [
    "fns_faceA = glob(dir_faceA + \"/*.*\")\n",
    "fns_faceB = glob(dir_faceB + \"/*.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3pciq80EvBWW"
   },
   "outputs": [],
   "source": [
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Blnjf0e-vBTU"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p binary_masks/faceA_eyes\n",
    "Path(\"binary_masks/faceA_eyes\").mkdir(parents=True, exist_ok=True)\n",
    "# !mkdir -p binary_masks/faceB_eyes\n",
    "Path(\"binary_masks/faceB_eyes\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zIucCV8uvBQr"
   },
   "outputs": [],
   "source": [
    "fns_face_not_detected = []\n",
    "\n",
    "for idx, fns in enumerate([fns_faceA, fns_faceB]):\n",
    "    if idx == 0:\n",
    "        save_path = dir_bm_faceA_eyes\n",
    "    elif idx == 1:\n",
    "        save_path = dir_bm_faceB_eyes     \n",
    "    \n",
    "    # create binary mask for each training image\n",
    "    for fn in tqdm(fns):\n",
    "        raw_fn = PurePath(fn).parts[-1]\n",
    "\n",
    "        x = plt.imread(fn)\n",
    "        x = cv2.resize(x, (256,256))\n",
    "        preds = fa.get_landmarks(x)\n",
    "        \n",
    "        if preds is not None:\n",
    "            preds = preds[0]\n",
    "            mask = np.zeros_like(x)\n",
    "            \n",
    "            # Draw right eye binary mask\n",
    "            pnts_right = [(preds[i,0],preds[i,1]) for i in range(36,42)]\n",
    "            hull = cv2.convexHull(np.array(pnts_right)).astype(np.int32)\n",
    "            mask = cv2.drawContours(mask,[hull],0,(255,255,255),-1)\n",
    "\n",
    "            # Draw left eye binary mask\n",
    "            pnts_left = [(preds[i,0],preds[i,1]) for i in range(42,48)]\n",
    "            hull = cv2.convexHull(np.array(pnts_left)).astype(np.int32)\n",
    "            mask = cv2.drawContours(mask,[hull],0,(255,255,255),-1)\n",
    "\n",
    "            # Draw mouth binary mask\n",
    "            #pnts_mouth = [(preds[i,0],preds[i,1]) for i in range(48,60)]\n",
    "            #hull = cv2.convexHull(np.array(pnts_mouth)).astype(np.int32)\n",
    "            #mask = cv2.drawContours(mask,[hull],0,(255,255,255),-1)\n",
    "            \n",
    "            mask = cv2.dilate(mask, np.ones((13,13), np.uint8), iterations=1)\n",
    "            mask = cv2.GaussianBlur(mask, (7,7), 0)\n",
    "            plt.imsave(fname=save_path+'/'+raw_fn, arr=mask, format=\"jpg\")\n",
    "            \n",
    "        else:\n",
    "            #mask = np.zeros_like(x)\n",
    "            print(\"No faces were detected in image \" + fn + \", deleting.\")\n",
    "            os.remove(fn)\n",
    "            fns_face_not_detected.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USPQzOB93Yxs"
   },
   "outputs": [],
   "source": [
    "num_faceA = len(glob(dir_faceA+\"/*.*\"))\n",
    "num_faceB = len(glob(dir_faceB+\"/*.*\"))\n",
    "\n",
    "print(\"Nuber of processed images: \"+ str(num_faceA + num_faceB))\n",
    "print(\"Number of image(s) with no face detected: \" + str(len(fns_face_not_detected)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7we-js5Q3cn4"
   },
   "source": [
    "# Training\n",
    "\n",
    "Training the model on the two chosen faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lV2AxBjc4EwY"
   },
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tsry7slM3hdB"
   },
   "outputs": [],
   "source": [
    "# Number of CPU cores\n",
    "num_cpus = os.cpu_count()\n",
    "\n",
    "# Input/Output resolution\n",
    "RESOLUTION = 128 # 64x64, 128x128, 256x256\n",
    "assert (RESOLUTION % 64) == 0, \"RESOLUTION should be 64, 128, or 256.\"\n",
    "\n",
    "# Batch size\n",
    "batchSize = 8\n",
    "assert (batchSize != 1 and batchSize % 2 == 0) , \"batchSize should be an even number.\"\n",
    "\n",
    "# Use motion blurs (data augmentation)\n",
    "# set True if training data contains images extracted from videos\n",
    "use_da_motion_blur = True \n",
    "\n",
    "# Use eye-aware training\n",
    "# require images generated from prep_binary_masks.ipynb\n",
    "use_bm_eyes = True\n",
    "\n",
    "# Probability of random color matching (data augmentation)\n",
    "prob_random_color_match = 0.5\n",
    "\n",
    "da_config = {\n",
    "    \"prob_random_color_match\": prob_random_color_match,\n",
    "    \"use_da_motion_blur\": use_da_motion_blur,\n",
    "    \"use_bm_eyes\": use_bm_eyes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cGrJdeOi4L5m"
   },
   "outputs": [],
   "source": [
    "# Path to training images\n",
    "img_dirA = './faceA'\n",
    "img_dirB = './faceB'\n",
    "img_dirA_bm_eyes = \"./binary_masks/faceA_eyes\"\n",
    "img_dirB_bm_eyes = \"./binary_masks/faceB_eyes\"\n",
    "\n",
    "# Path to saved model weights\n",
    "models_dir = \"./models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pNOOHk8B4N2-"
   },
   "outputs": [],
   "source": [
    "# Architecture configuration\n",
    "arch_config = {}\n",
    "arch_config['IMAGE_SHAPE'] = (RESOLUTION, RESOLUTION, 3)\n",
    "arch_config['use_self_attn'] = True\n",
    "arch_config['norm'] = \"instancenorm\" # instancenorm, batchnorm, layernorm, groupnorm, none\n",
    "arch_config['model_capacity'] = \"standard\" # standard, lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aM5ZuZ584N0_"
   },
   "outputs": [],
   "source": [
    "# Loss function weights configuration\n",
    "loss_weights = {}\n",
    "loss_weights['w_D'] = 0.1 # Discriminator\n",
    "loss_weights['w_recon'] = 1. # L1 reconstruction loss\n",
    "loss_weights['w_edge'] = 0.1 # edge loss\n",
    "loss_weights['w_eyes'] = 30. # reconstruction and edge loss on eyes area\n",
    "loss_weights['w_pl'] = (0.01, 0.1, 0.3, 0.1) # perceptual loss (0.003, 0.03, 0.3, 0.3)\n",
    "\n",
    "# Init. loss config.\n",
    "loss_config = {}\n",
    "loss_config[\"gan_training\"] = \"mixup_LSGAN\" # \"mixup_LSGAN\" or \"relativistic_avg_LSGAN\"\n",
    "loss_config['use_PL'] = False\n",
    "loss_config[\"PL_before_activ\"] = False\n",
    "loss_config['use_mask_hinge_loss'] = False\n",
    "loss_config['m_mask'] = 0.\n",
    "loss_config['lr_factor'] = 1.\n",
    "loss_config['use_cyclic_loss'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rpa6kufl4ROw"
   },
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PM323liU4NzI"
   },
   "outputs": [],
   "source": [
    "from networks.faceswap_gan_model import FaceswapGANModel\n",
    "model = FaceswapGANModel(**arch_config)\n",
    "model.load_weights(path=models_dir+\"/backup_iter10000\")\n",
    "\n",
    "from keras_vggface.vggface import VGGFace\n",
    "\n",
    "# VGGFace ResNet50\n",
    "vggface = VGGFace(include_top=False, model='resnet50', input_shape=(224, 224, 3))\n",
    "model.build_pl_model(vggface_model=vggface, before_activ=loss_config[\"PL_before_activ\"])\n",
    "\n",
    "model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "\n",
    "# Create ./models directory\n",
    "Path(\"models\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kxY_SMe-4_Q8"
   },
   "outputs": [],
   "source": [
    "from data_loader.data_loader import DataLoader\n",
    "\n",
    "def show_loss_config(loss_config):\n",
    "    for config, value in loss_config.items():\n",
    "        print(config + ' = ' + str(value))\n",
    "\n",
    "def reset_session(save_path):\n",
    "    global model, vggface\n",
    "    global train_batchA, train_batchB\n",
    "    model.save_weights(path=save_path)\n",
    "    del model\n",
    "    del vggface\n",
    "    del train_batchA\n",
    "    del train_batchB\n",
    "    K.clear_session()\n",
    "    model = FaceswapGANModel(**arch_config)\n",
    "    model.load_weights(path=save_path)\n",
    "    vggface = VGGFace(include_top=False, model='resnet50', input_shape=(224, 224, 3))\n",
    "    model.build_pl_model(vggface_model=vggface, before_activ=loss_config[\"PL_before_activ\"])\n",
    "    train_batchA = DataLoader(train_A, train_AnB, batchSize, img_dirA_bm_eyes,\n",
    "                              RESOLUTION, num_cpus, K.get_session(), **da_config)\n",
    "    train_batchB = DataLoader(train_B, train_AnB, batchSize, img_dirB_bm_eyes, \n",
    "                              RESOLUTION, num_cpus, K.get_session(), **da_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f-MgR2484xqh"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11478,
     "status": "ok",
     "timestamp": 1570073238895,
     "user": {
      "displayName": "Deep Learning",
      "photoUrl": "",
      "userId": "02790113473800056642"
     },
     "user_tz": -330
    },
    "id": "DiQa774_46MQ",
    "outputId": "632b6865-eac2-4b36-c56c-58e44046e2c8"
   },
   "outputs": [],
   "source": [
    "from utils import showG, showG_mask, showG_eyes\n",
    "\n",
    "# Get filenames\n",
    "train_A = glob(img_dirA+\"/*.*\")\n",
    "train_B = glob(img_dirB+\"/*.*\")\n",
    "\n",
    "train_AnB = train_A + train_B\n",
    "\n",
    "assert len(train_A), \"No image found in \" + str(img_dirA)\n",
    "assert len(train_B), \"No image found in \" + str(img_dirB)\n",
    "print (\"Number of images in folder A: \" + str(len(train_A)))\n",
    "print (\"Number of images in folder B: \" + str(len(train_B)))\n",
    "\n",
    "if use_bm_eyes:\n",
    "    assert len(glob(img_dirA_bm_eyes+\"/*.*\")), \"No binary mask found in \" + str(img_dirA_bm_eyes)\n",
    "    assert len(glob(img_dirB_bm_eyes+\"/*.*\")), \"No binary mask found in \" + str(img_dirB_bm_eyes)\n",
    "    assert len(glob(img_dirA_bm_eyes+\"/*.*\")) == len(train_A), \\\n",
    "    \"Number of faceA images does not match number of their binary masks. Can be caused by any none image file in the folder.\"\n",
    "    assert len(glob(img_dirB_bm_eyes+\"/*.*\")) == len(train_B), \\\n",
    "    \"Number of faceB images does not match number of their binary masks. Can be caused by any none image file in the folder.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 940
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68364,
     "status": "ok",
     "timestamp": 1570073307264,
     "user": {
      "displayName": "Deep Learning",
      "photoUrl": "",
      "userId": "02790113473800056642"
     },
     "user_tz": -330
    },
    "id": "GHdrYflI49Jb",
    "outputId": "0ad8e5c2-348f-42ec-f5e0-db43df94a974"
   },
   "outputs": [],
   "source": [
    "# Display random binary masks of eyes\n",
    "train_batchA = DataLoader(train_A, train_AnB, batchSize, img_dirA_bm_eyes, \n",
    "                          RESOLUTION, num_cpus, K.get_session(), **da_config)\n",
    "train_batchB = DataLoader(train_B, train_AnB, batchSize, img_dirB_bm_eyes, \n",
    "                          RESOLUTION, num_cpus, K.get_session(), **da_config)\n",
    "_, tA, bmA = train_batchA.get_next_batch()\n",
    "_, tB, bmB = train_batchB.get_next_batch()\n",
    "showG_eyes(tA, tB, bmA, bmB, batchSize)\n",
    "del train_batchA, train_batchB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to send updates to e-mail account of your choice\n",
    "import smtplib \n",
    "from email.mime.multipart import MIMEMultipart \n",
    "from email.mime.text import MIMEText \n",
    "from email.mime.base import MIMEBase \n",
    "from email import encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fromaddr = \"example@gmail.com\"\n",
    "toaddr = \"example@gmail.com\"\n",
    "frompassword = 'password123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YeEA4oUS5i2-",
    "outputId": "a4d268d6-f0da-431b-8cc3-c8e20f48c96b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "t0 = time.time()\n",
    "\n",
    "# This try/except is meant to resume training that was accidentally interrupted\n",
    "try:\n",
    "    gen_iterations\n",
    "    print(\"Resume training from iter \"+str(gen_iterations))\n",
    "except:\n",
    "    gen_iterations = 0\n",
    "print('Initializing Errors...')\n",
    "errGA_sum = errGB_sum = errDA_sum = errDB_sum = 0\n",
    "errGAs = {}\n",
    "errGBs = {}\n",
    "# Dictionaries are ordered in Python 3.6\n",
    "for k in ['ttl', 'adv', 'recon', 'edge', 'pl']:\n",
    "    errGAs[k] = 0\n",
    "    errGBs[k] = 0\n",
    "\n",
    "display_iters = 5000 # Display results every {display_iters} iterations\n",
    "backup_iters = 5000 # Backup models every {backup_iters} iterations\n",
    "TOTAL_ITERS = 50000 # Train model for {TOTAL_ITERS} iterations\n",
    "\n",
    "print('Creating Dataloaders...')\n",
    "global train_batchA, train_batchB\n",
    "train_batchA = DataLoader(train_A, train_AnB, batchSize, img_dirA_bm_eyes, \n",
    "                          RESOLUTION, num_cpus, K.get_session(), **da_config)\n",
    "train_batchB = DataLoader(train_B, train_AnB, batchSize, img_dirB_bm_eyes, \n",
    "                          RESOLUTION, num_cpus, K.get_session(), **da_config)\n",
    "\n",
    "print('Start training...')\n",
    "while gen_iterations <= TOTAL_ITERS: \n",
    "    \n",
    "    # Loss function automation\n",
    "    if gen_iterations == (TOTAL_ITERS//5 - display_iters//2):\n",
    "        clear_output()\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = False\n",
    "        loss_config['m_mask'] = 0.0\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Done.\")\n",
    "    elif gen_iterations == (TOTAL_ITERS//5 + TOTAL_ITERS//10 - display_iters//2):\n",
    "        clear_output()\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = True\n",
    "        loss_config['m_mask'] = 0.5\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Complete.\")\n",
    "    elif gen_iterations == (2*TOTAL_ITERS//5 - display_iters//2):\n",
    "        clear_output()\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = True\n",
    "        loss_config['m_mask'] = 0.2\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Done.\")\n",
    "    elif gen_iterations == (TOTAL_ITERS//2 - display_iters//2):\n",
    "        clear_output()\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = True\n",
    "        loss_config['m_mask'] = 0.4\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Done.\")\n",
    "    elif gen_iterations == (2*TOTAL_ITERS//3 - display_iters//2):\n",
    "        clear_output()\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = False\n",
    "        loss_config['m_mask'] = 0.\n",
    "        loss_config['lr_factor'] = 0.3\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Done.\")\n",
    "    elif gen_iterations == (8*TOTAL_ITERS//10 - display_iters//2):\n",
    "        clear_output()\n",
    "        model.decoder_A.load_weights(\"models/decoder_B.h5\") # swap decoders\n",
    "        model.decoder_B.load_weights(\"models/decoder_A.h5\") # swap decoders\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = True\n",
    "        loss_config['m_mask'] = 0.1\n",
    "        loss_config['lr_factor'] = 0.3\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Done.\")\n",
    "    elif gen_iterations == (9*TOTAL_ITERS//10 - display_iters//2):\n",
    "        clear_output()\n",
    "        loss_config['use_PL'] = True\n",
    "        loss_config['use_mask_hinge_loss'] = False\n",
    "        loss_config['m_mask'] = 0.0\n",
    "        loss_config['lr_factor'] = 0.1\n",
    "        reset_session(models_dir)\n",
    "        print(\"Building new loss funcitons...\")\n",
    "        show_loss_config(loss_config)\n",
    "        model.build_train_functions(loss_weights=loss_weights, **loss_config)\n",
    "        print(\"Done.\")\n",
    "    \n",
    "    if gen_iterations == 5:\n",
    "        print (\"working.\")\n",
    "    \n",
    "    # Train dicriminators for one batch\n",
    "    data_A = train_batchA.get_next_batch()\n",
    "    data_B = train_batchB.get_next_batch()\n",
    "    errDA, errDB = model.train_one_batch_D(data_A=data_A, data_B=data_B)\n",
    "    errDA_sum +=errDA[0]\n",
    "    errDB_sum +=errDB[0]\n",
    "\n",
    "    # Train generators for one batch\n",
    "    data_A = train_batchA.get_next_batch()\n",
    "    data_B = train_batchB.get_next_batch()\n",
    "    errGA, errGB = model.train_one_batch_G(data_A=data_A, data_B=data_B)\n",
    "    errGA_sum += errGA[0]\n",
    "    errGB_sum += errGB[0]\n",
    "    \"\"\"for i, k in enumerate(['ttl', 'adv', 'recon', 'edge', 'pl']):\n",
    "        errGAs[k] += errGA[i]\n",
    "        errGBs[k] += errGB[i]\"\"\"\n",
    "    gen_iterations+=1\n",
    "    \n",
    "    if gen_iterations % (display_iters//10) == 0:\n",
    "        print(str(gen_iterations) + ' iterations done in ' + str(time.time()-t0))\n",
    "    \n",
    "    # Visualization\n",
    "    if gen_iterations % display_iters == 0:\n",
    "        clear_output()\n",
    "        \n",
    "        #####\n",
    "        msg = MIMEMultipart()  \n",
    "        msg['From'] = fromaddr \n",
    "        msg['To'] = toaddr \n",
    "        msg['Subject'] = vid1_name[:-4] + ' and ' + vid2_name[:-4] + \": Epochs \" + str(gen_iterations)\n",
    "        body = \"\"\n",
    "        msg.attach(MIMEText(body, 'plain'))\n",
    "        #####\n",
    "        \n",
    "        # Display loss information\n",
    "        show_loss_config(loss_config)\n",
    "        print(\"----------\") \n",
    "        print('[iter %d] Loss_DA: %f Loss_DB: %f Loss_GA: %f Loss_GB: %f time: %f'\n",
    "        % (gen_iterations, errDA_sum/display_iters, errDB_sum/display_iters,\n",
    "           errGA_sum/display_iters, errGB_sum/display_iters, time.time()-t0))  \n",
    "        print(\"----------\") \n",
    "        print(\"Generator loss details:\")\n",
    "        print('[Adversarial loss]')  \n",
    "        print('GA: {:.4f} GB: {:.4f}'.format(errGAs[\"adv\"]/display_iters, errGBs[\"adv\"]/display_iters))\n",
    "        print('[Reconstruction loss]')\n",
    "        print('GA: {:.4f} GB: {:.4f}'.format(errGAs[\"recon\"]/display_iters, errGBs[\"recon\"]/display_iters))\n",
    "        print('[Edge loss]')\n",
    "        print('GA: {:.4f} GB: {:.4f}'.format(errGAs[\"edge\"]/display_iters, errGBs[\"edge\"]/display_iters))\n",
    "        if loss_config['use_PL'] == True:\n",
    "            print('[Perceptual loss]')\n",
    "            try:\n",
    "                print('GA: {:.4f} GB: {:.4f}'.format(errGAs[\"pl\"][0]/display_iters, errGBs[\"pl\"][0]/display_iters))\n",
    "            except:\n",
    "                print('GA: {:.4f} GB: {:.4f}'.format(errGAs[\"pl\"]/display_iters, errGBs[\"pl\"]/display_iters))\n",
    "        \n",
    "        # Display images\n",
    "        print(\"----------\") \n",
    "        wA, tA, _ = train_batchA.get_next_batch()\n",
    "        wB, tB, _ = train_batchB.get_next_batch()\n",
    "        \n",
    "        print(\"Transformed (masked) results:\")\n",
    "        showG(tA, tB, model.path_A, model.path_B, batchSize)  \n",
    "        filename = \"opimages/showG.jpg\"\n",
    "        attachment = open(filename, \"rb\") \n",
    "        # instance of MIMEBase and named as p \n",
    "        p = MIMEBase('application', 'octet-stream') \n",
    "        # To change the payload into encoded form \n",
    "        p.set_payload((attachment).read()) \n",
    "        # encode into base64 \n",
    "        encoders.encode_base64(p) \n",
    "        p.add_header('Content-Disposition', \"attachment; filename= showG.jpg\")\n",
    "        # attach the instance 'p' to instance 'msg' \n",
    "        msg.attach(p)\n",
    "        \n",
    "        print(\"Masks:\")\n",
    "        showG_mask(tA, tB, model.path_mask_A, model.path_mask_B, batchSize)\n",
    "        filename = \"opimages/showG_mask.jpg\"\n",
    "        attachment = open(filename, \"rb\") \n",
    "        # instance of MIMEBase and named as p \n",
    "        p = MIMEBase('application', 'octet-stream') \n",
    "        # To change the payload into encoded form \n",
    "        p.set_payload((attachment).read()) \n",
    "        # encode into base64 \n",
    "        encoders.encode_base64(p) \n",
    "        p.add_header('Content-Disposition', \"attachment; filename= showG_mask.jpg\")\n",
    "        # attach the instance 'p' to instance 'msg' \n",
    "        msg.attach(p)\n",
    "        \n",
    "        print(\"Reconstruction results:\")\n",
    "        showG(wA, wB, model.path_bgr_A, model.path_bgr_B, batchSize)\n",
    "        filename = \"opimages/showG.jpg\"\n",
    "        attachment = open(filename, \"rb\") \n",
    "        # instance of MIMEBase and named as p \n",
    "        p = MIMEBase('application', 'octet-stream') \n",
    "        # To change the payload into encoded form \n",
    "        p.set_payload((attachment).read()) \n",
    "        # encode into base64 \n",
    "        encoders.encode_base64(p) \n",
    "        p.add_header('Content-Disposition', \"attachment; filename= showG1.jpg\") \n",
    "        # attach the instance 'p' to instance 'msg' \n",
    "        msg.attach(p)\n",
    "        \n",
    "        # creates SMTP session \n",
    "        s = smtplib.SMTP('smtp.gmail.com', 587) \n",
    "        # start TLS for security \n",
    "        s.starttls() \n",
    "        # Authentication \n",
    "        s.login(fromaddr, frompassword)\n",
    "        # Converts the Multipart msg into a string \n",
    "        text = msg.as_string() \n",
    "        # sending the mail \n",
    "        s.sendmail(fromaddr, toaddr, text)\n",
    "        # terminating the session \n",
    "        s.quit()\n",
    "        \n",
    "        errGA_sum = errGB_sum = errDA_sum = errDB_sum = 0\n",
    "        for k in ['ttl', 'adv', 'recon', 'edge', 'pl']:\n",
    "            errGAs[k] = 0\n",
    "            errGBs[k] = 0\n",
    "        \n",
    "        # Save models\n",
    "        model.save_weights(path=models_dir)\n",
    "\n",
    "    # Backup models\n",
    "    if gen_iterations % backup_iters == 0: \n",
    "        bkup_dir = \"{}/backup_iter{}\".format(models_dir, gen_iterations)\n",
    "        Path(bkup_dir).mkdir(parents=True, exist_ok=True)\n",
    "        model.save_weights(path=bkup_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PD0zplEF5szt"
   },
   "outputs": [],
   "source": [
    "# Display random results\n",
    "wA, tA, _ = train_batchA.get_next_batch()\n",
    "wB, tB, _ = train_batchB.get_next_batch()\n",
    "print(\"Transformed (masked) results:\")\n",
    "showG(tA, tB, model.path_A, model.path_B, batchSize)   \n",
    "print(\"Masks:\")\n",
    "showG_mask(tA, tB, model.path_mask_A, model.path_mask_B, batchSize)  \n",
    "print(\"Reconstruction results:\")\n",
    "showG(wA, wB, model.path_bgr_A, model.path_bgr_B, batchSize) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cThG6uB_6BKh"
   },
   "source": [
    "# Video Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UnPOJlDelo6y"
   },
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import time\n",
    "from pathlib import PurePath, Path\n",
    "from glob import glob\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras import regularizers\n",
    "from moviepy.editor import VideoFileClip\n",
    "from tqdm import tqdm\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('faceswap-GAN/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPwxbS5olo66"
   },
   "outputs": [],
   "source": [
    "K.set_learning_phase(0)\n",
    "\n",
    "# Input/Output resolution\n",
    "RESOLUTION = 128 # 64x64, 128x128, 256x256\n",
    "assert (RESOLUTION % 64) == 0, \"RESOLUTION should be 64, 128, 256\"\n",
    "\n",
    "# Architecture configuration\n",
    "arch_config = {}\n",
    "arch_config['IMAGE_SHAPE'] = (RESOLUTION, RESOLUTION, 3)\n",
    "arch_config['use_self_attn'] = True\n",
    "arch_config['norm'] = \"instancenorm\" # instancenorm, batchnorm, layernorm, groupnorm, none\n",
    "arch_config['model_capacity'] = \"standard\" # standard, lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tUpqEw9ulo69"
   },
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9qlayPXMlo7B",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from networks.faceswap_gan_model import FaceswapGANModel\n",
    "model = FaceswapGANModel(**arch_config)\n",
    "model.load_weights(path=\"./models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "89SckvValo7L"
   },
   "source": [
    "## Video Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-k6KmB6Jlo7L"
   },
   "outputs": [],
   "source": [
    "from converter.video_converter import VideoConverter\n",
    "from detector.face_detector import MTCNNFaceDetector\n",
    "\n",
    "mtcnn_weights_dir = \"./mtcnn_weights/\"\n",
    "\n",
    "fd = MTCNNFaceDetector(sess=K.get_session(), model_path=mtcnn_weights_dir)\n",
    "vc = VideoConverter()\n",
    "\n",
    "vc.set_face_detector(fd)\n",
    "vc.set_gan_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir() if 'mp4' in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NKgnFMM7lo7V"
   },
   "source": [
    "### Video conversion configuration\n",
    "\n",
    "\n",
    "- `use_smoothed_bbox`: \n",
    "    - Boolean. Whether to enable smoothed bbox.\n",
    "- `use_kalman_filter`: \n",
    "    - Boolean. Whether to enable Kalman filter.\n",
    "- `use_auto_downscaling`:\n",
    "    - Boolean. Whether to enable auto-downscaling in face detection (to prevent OOM error).\n",
    "- `bbox_moving_avg_coef`: \n",
    "    - Float point between 0 and 1. Smoothing coef. used when use_kalman_filter is set False.\n",
    "- `min_face_area`:\n",
    "    - int x int. Minimum size of face. Detected faces smaller than min_face_area will not be transformed.\n",
    "- `IMAGE_SHAPE`:\n",
    "    - Input/Output resolution of the GAN model\n",
    "- `kf_noise_coef`:\n",
    "    - Float point. Increase by 10x if tracking is slow. Decrease by 1/10x if trakcing works fine but jitter occurs.\n",
    "- `use_color_correction`: \n",
    "    - String of \"adain\", \"adain_xyz\", \"hist_match\", or \"none\". The color correction method to be applied.\n",
    "- `detec_threshold`: \n",
    "    - Float point between 0 and 1. Decrease its value if faces are missed. Increase its value to reduce false positives.\n",
    "- `roi_coverage`: \n",
    "    - Float point between 0 and 1 (exclusive). Center area of input images to be cropped (Suggested range: 0.85 ~ 0.95)\n",
    "- `enhance`: \n",
    "    - Float point. A coef. for contrast enhancement in the region of alpha mask (Suggested range: 0. ~ 0.4)\n",
    "- `output_type`: \n",
    "    - Layout format of output video: 1. [ result ], 2. [ source | result ], 3. [ source | result | mask ]\n",
    "- `direction`: \n",
    "    - String of \"AtoB\" or \"BtoA\". Direction of face transformation.\n",
    "\n",
    "### Start video conversion\n",
    "\n",
    "\n",
    "- `input_fn`: \n",
    "    - String. Input video path.\n",
    "- `output_fn`: \n",
    "    - String. Output video path.\n",
    "- `duration`: \n",
    "    - None or a non-negative float tuple: (start_sec, end_sec). Duration of input video to be converted\n",
    "    - e.g., setting `duration = (5, 7.5)` outputs a 2.5-sec-long video clip corresponding to 5s ~ 7.5s of the input video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid1_name = 'ankit.mp4'\n",
    "vid2_name = 'vaibhav.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TdIectf6lo7W"
   },
   "outputs": [],
   "source": [
    "options = {\n",
    "    # ===== Fixed =====\n",
    "    \"use_smoothed_bbox\": True,\n",
    "    \"use_kalman_filter\": True,\n",
    "    \"use_auto_downscaling\": False,\n",
    "    \"bbox_moving_avg_coef\": 0.65,\n",
    "    \"min_face_area\": 35 * 35,\n",
    "    \"IMAGE_SHAPE\": model.IMAGE_SHAPE,\n",
    "    # ===== Tunable =====\n",
    "    \"kf_noise_coef\": 3e-3,\n",
    "    \"use_color_correction\": \"hist_match\",\n",
    "    \"detec_threshold\": 0.7,\n",
    "    \"roi_coverage\": 0.9,\n",
    "    \"enhance\": 0.,\n",
    "    \"output_type\": 1,\n",
    "    \"direction\": \"AtoB\",\n",
    "}\n",
    "\n",
    "input_fn = vid1_name\n",
    "output_fn = vid1_name.split('.')[0] + '2' + vid2_name.split('.')[0] +'_35_35.mp4'\n",
    "duration = None\n",
    "vc.convert(input_fn=input_fn, output_fn=output_fn, options=options, duration=duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6luieJSClo7a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options = {\n",
    "    # ===== Fixed =====\n",
    "    \"use_smoothed_bbox\": True,\n",
    "    \"use_kalman_filter\": True,\n",
    "    \"use_auto_downscaling\": False,\n",
    "    \"bbox_moving_avg_coef\": 0.65,\n",
    "    \"min_face_area\": 35 * 35,\n",
    "    \"IMAGE_SHAPE\": model.IMAGE_SHAPE,\n",
    "    # ===== Tunable =====\n",
    "    \"kf_noise_coef\": 3e-3,\n",
    "    \"use_color_correction\": \"hist_match\",\n",
    "    \"detec_threshold\": 0.7,\n",
    "    \"roi_coverage\": 0.9,\n",
    "    \"enhance\": 0.,\n",
    "    \"output_type\": 1,\n",
    "    \"direction\": \"BtoA\",\n",
    "}\n",
    "\n",
    "input_fn = vid2_name\n",
    "output_fn = vid2_name.split('.')[0] + '2' + vid1_name.split('.')[0] + '_35_35.mp4'\n",
    "duration = None\n",
    "\n",
    "vc.convert(input_fn=input_fn, output_fn=output_fn, options=options, duration=duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ebLjlCkBkueH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "UnPOJlDelo6y",
    "tUpqEw9ulo69",
    "89SckvValo7L",
    "NKgnFMM7lo7V"
   ],
   "name": "FaceSwap_GAN_full_colab.ipynb",
   "provenance": [
    {
     "file_id": "1MtKkr1gpBQhtZ7h_PxZIv9IegCkrPLOl",
     "timestamp": 1570037978677
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
