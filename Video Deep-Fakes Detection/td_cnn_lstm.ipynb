{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "toDWTe_M4JYO"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "import tensorflow as tf\n",
    "from random import shuffle\n",
    "from tqdm import tqdm_notebook\n",
    "import itertools\n",
    "import cv2\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PryKlRiptDW0"
   },
   "outputs": [],
   "source": [
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGpVdyzKYBc5"
   },
   "outputs": [],
   "source": [
    "dataset = 'PATH TO DATASET'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0CuxEQOw8YX"
   },
   "outputs": [],
   "source": [
    "num_images = 10\n",
    "img_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "BKwho3TWIjkg",
    "outputId": "27c8bb44-6a52-4ae1-b249-6b89b6bec788"
   },
   "outputs": [],
   "source": [
    "print('\\nVideo:')\n",
    "for split in ['train','val','test']:\n",
    "    for label in ['real','fake']:\n",
    "        folders = [f for f in os.listdir(f'{dataset}/{split}/{label}') if len([j for j in os.listdir(f'{dataset}/{split}/{label}/{f}') if 'jpg' in j]) == num_images and '.ipynb' not in f]\n",
    "        print(f'{split}_{label}: {len(folders)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vUFtyuWE5t5x"
   },
   "outputs": [],
   "source": [
    "def Xception(input_shape=(None,None,None,3), lr = 1e-3):\n",
    "    channel_axis = 3\n",
    "    img_input = layers.Input(input_shape)\n",
    "    x = layers.TimeDistributed(layers.Conv2D(32, (3, 3), strides=(2, 2), use_bias=False, name='block1_conv1'))(img_input)\n",
    "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block1_conv1_bn'))(x)\n",
    "    x = layers.TimeDistributed(layers.Activation('relu', name='block1_conv1_act'))(x)\n",
    "    x = layers.TimeDistributed(layers.Conv2D(64, (3, 3), use_bias=False, name='block1_conv2'))(x)\n",
    "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block1_conv2_bn'))(x)\n",
    "    x = layers.TimeDistributed(layers.Activation('relu', name='block1_conv2_act'))(x)\n",
    "\n",
    "    residual = layers.TimeDistributed(layers.Conv2D(128, (1, 1), strides=(2, 2), padding='same', use_bias=False))(x)\n",
    "    residual = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis))(residual)\n",
    "\n",
    "    x = layers.TimeDistributed(layers.SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv1'))(x)\n",
    "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block2_sepconv1_bn'))(x)\n",
    "    x = layers.TimeDistributed(layers.Activation('relu', name='block2_sepconv2_act'))(x)\n",
    "    x = layers.TimeDistributed(layers.SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv2'))(x)\n",
    "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block2_sepconv2_bn'))(x)\n",
    "\n",
    "    x = layers.TimeDistributed(layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block2_pool'))(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    residual = layers.TimeDistributed(layers.Conv2D(256, (1, 1), strides=(2, 2), padding='same', use_bias=False))(x)\n",
    "    residual = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis))(residual)\n",
    "\n",
    "    x = layers.TimeDistributed(layers.Activation('relu', name='block3_sepconv1_act'))(x)\n",
    "    x = layers.TimeDistributed(layers.SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv1'))(x)\n",
    "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block3_sepconv1_bn'))(x)\n",
    "    x = layers.TimeDistributed(layers.Activation('relu', name='block3_sepconv2_act'))(x)\n",
    "    x = layers.TimeDistributed(layers.SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv2'))(x)\n",
    "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block3_sepconv2_bn'))(x)\n",
    "\n",
    "    x = layers.TimeDistributed(layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block3_pool'))(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    residual = layers.TimeDistributed(layers.Conv2D(728, (1, 1), strides=(2, 2), padding='same', use_bias=False))(x)\n",
    "    residual = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis))(residual)\n",
    "\n",
    "    x = layers.TimeDistributed(layers.Activation('relu', name='block4_sepconv1_act'))(x)\n",
    "    x = layers.TimeDistributed(layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv1'))(x)\n",
    "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block4_sepconv1_bn'))(x)\n",
    "    x = layers.TimeDistributed(layers.Activation('relu', name='block4_sepconv2_act'))(x)\n",
    "    x = layers.TimeDistributed(layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv2'))(x)\n",
    "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block4_sepconv2_bn'))(x)\n",
    "\n",
    "    x = layers.TimeDistributed(layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block4_pool'))(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    for i in range(8):\n",
    "        residual = x\n",
    "        prefix = 'block' + str(i + 5)\n",
    "\n",
    "        x = layers.TimeDistributed(layers.Activation('relu', name=prefix + '_sepconv1_act'))(x)\n",
    "        x = layers.TimeDistributed(layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv1'))(x)\n",
    "        x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name=prefix + '_sepconv1_bn'))(x)\n",
    "        x = layers.TimeDistributed(layers.Activation('relu', name=prefix + '_sepconv2_act'))(x)\n",
    "        x = layers.TimeDistributed(layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv2'))(x)\n",
    "        x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name=prefix + '_sepconv2_bn'))(x)\n",
    "        x = layers.TimeDistributed(layers.Activation('relu', name=prefix + '_sepconv3_act'))(x)\n",
    "        x = layers.TimeDistributed(layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv3'))(x)\n",
    "        x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name=prefix + '_sepconv3_bn'))(x)\n",
    "\n",
    "        x = layers.add([x, residual])\n",
    "\n",
    "    residual = layers.TimeDistributed(layers.Conv2D(1024, (1, 1), strides=(2, 2), padding='same', use_bias=False))(x)\n",
    "    residual = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis))(residual)\n",
    "\n",
    "    x = layers.TimeDistributed(layers.Activation('relu', name='block13_sepconv1_act'))(x)\n",
    "    x = layers.TimeDistributed(layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block13_sepconv1'))(x)\n",
    "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block13_sepconv1_bn'))(x)\n",
    "    x = layers.TimeDistributed(layers.Activation('relu', name='block13_sepconv2_act'))(x)\n",
    "    x = layers.TimeDistributed(layers.SeparableConv2D(1024, (3, 3), padding='same', use_bias=False, name='block13_sepconv2'))(x)\n",
    "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block13_sepconv2_bn'))(x)\n",
    "\n",
    "    x = layers.TimeDistributed(layers.MaxPooling2D((3, 3),\n",
    "                            strides=(2, 2),\n",
    "                            padding='same',\n",
    "                            name='block13_pool'))(x)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "    x = layers.TimeDistributed(layers.SeparableConv2D(1536, (3, 3), padding='same', use_bias=False, name='block14_sepconv1'))(x)\n",
    "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block14_sepconv1_bn'))(x)\n",
    "    x = layers.TimeDistributed(layers.Activation('relu', name='block14_sepconv1_act'))(x)\n",
    "\n",
    "    x = layers.TimeDistributed(layers.SeparableConv2D(2048, (3, 3), padding='same', use_bias=False, name='block14_sepconv2'))(x)\n",
    "    x = layers.TimeDistributed(layers.BatchNormalization(axis=channel_axis, name='block14_sepconv2_bn'))(x)\n",
    "    x = layers.TimeDistributed(layers.Activation('relu', name='block14_sepconv2_act'))(x)\n",
    "\n",
    "    x = layers.TimeDistributed(layers.GlobalAveragePooling2D(name='avg_pool'))(x)\n",
    "    \n",
    "    model = models.Model(img_input, x, name='xception')\n",
    "    model.compile(optimizer = Adam(lr), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_callbacks(modelname, es_patience, rlr_patience, verbose = 1):\n",
    "    checkpointer = ModelCheckpoint(modelname+'.h5', monitor = 'val_loss', save_best_only = True, verbose = verbose, mode = 'min')\n",
    "    earlystopper = EarlyStopping(monitor = 'val_loss', patience = es_patience, verbose = verbose, mode = 'min')\n",
    "    reduceLR = ReduceLROnPlateau(monitor = 'val_loss', factor = 1/np.sqrt(10), patience = rlr_patience, cooldown = 1 ,verbose = verbose, mode = 'min')\n",
    "    return checkpointer, earlystopper, reduceLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4tNDKuFb7Wa1"
   },
   "outputs": [],
   "source": [
    "def generator(l, batch_size):\n",
    "    gen = iter(itertools.cycle(l))\n",
    "    while 1:\n",
    "        yield [next(gen) for _ in range(batch_size)]\n",
    "\n",
    "def get_input(dataset, split, label, folder):\n",
    "    images = np.zeros((num_images,img_size,img_size,3), dtype = np.float32)\n",
    "    imagespath = [f for f in os.listdir(f'{dataset}/{split}/{label}/{folder}') if 'jpg' in f]\n",
    "    for i in range(num_images):\n",
    "        img = cv2.resize(plt.imread(f'{dataset}/{split}/{label}/{folder}/{imagespath[i]}'), (img_size, img_size), interpolation=cv2.INTER_CUBIC)/255\n",
    "        images[i] = img\n",
    "    return [images]\n",
    "\n",
    "def video_data_generator(dataset, mode = 'train', batch_size = 8):\n",
    "    \n",
    "    if mode == 'test':\n",
    "        batch_size = batch_size // 4\n",
    "\n",
    "    assert batch_size % 2 == 0\n",
    "    \n",
    "    real_folders = [f for f in os.listdir(f'{dataset}/{mode}/real') if '.ipynb' not in f]\n",
    "    fake_folders = [f for f in os.listdir(f'{dataset}/{mode}/fake') if '.ipynb' not in f]\n",
    "    \n",
    "    shuffle(real_folders)\n",
    "    shuffle(fake_folders)\n",
    "    \n",
    "    real_gen = generator(real_folders, batch_size = batch_size // 2)\n",
    "    fake_gen = generator(fake_folders, batch_size = batch_size // 2)\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        real_folder_batch, fake_folder_batch = next(real_gen), next(fake_gen)\n",
    "        batch_input = []\n",
    "        batch_output = []\n",
    "        for real_folder, fake_folder in zip(real_folder_batch, fake_folder_batch):\n",
    "            \n",
    "            batch_input += get_input(dataset, mode, 'real', real_folder)\n",
    "            batch_input += get_input(dataset, mode, 'fake', fake_folder)\n",
    "            batch_output += [0., 1.]\n",
    "        # Return a tuple of (input,output) to feed the network\n",
    "        batch_x = np.array(batch_input)\n",
    "        batch_y = np.array(batch_output)\n",
    "\n",
    "        yield (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YnJZRdqR7P8I",
    "outputId": "49085ab2-20fa-46bb-e88c-c897fb1bec71"
   },
   "outputs": [],
   "source": [
    "# Video\n",
    "\n",
    "model_path = f'models/TDCNN_LSTM' # Save models here\n",
    "Path(model_path).mkdir(parents=True, exist_ok = True)\n",
    "\n",
    "epochs = 40\n",
    "batch_size = 16\n",
    "\n",
    "train_spe = len(os.listdir(f'{dataset}/train/real'))//batch_size\n",
    "print(train_spe)\n",
    "val_spe = len(os.listdir(f'{dataset}/val/real'))//batch_size\n",
    "print(val_spe)\n",
    "test_spe = len(os.listdir(f'{dataset}/test/real'))//(batch_size//4)\n",
    "print(test_spe)\n",
    "\n",
    "# Train multiple times\n",
    "for attempt in range(3):\n",
    "    print(f'Attempt #{attempt}')\n",
    "    foldername = f'{model_path}/{dataset}_xceptionimagenet_attempt{attempt}'\n",
    "    Path(foldername).mkdir(parents=True, exist_ok=True)\n",
    "    modelname = f'{foldername}/{dataset}_xceptionimagenet_{num_images}_{img_size}'\n",
    "\n",
    "    train_datagen = video_data_generator(dataset,'train', batch_size = batch_size)\n",
    "    val_datagen = video_data_generator(dataset,'val', batch_size = batch_size)\n",
    "    test_datagen = video_data_generator(dataset,'test', batch_size = batch_size)\n",
    "\n",
    "    base_model = Xception()\n",
    "    base_model.load_weights('imagenet')\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    inputs = layers.Input(shape=(None,None,None,3))\n",
    "    outputs = base_model(inputs, training=False)\n",
    "    outputs = layers.Dropout(0.5, name = 'lstm_dropout')(outputs)\n",
    "    outputs = layers.LSTM(1, dtype='float32', name='LSTM_False')(outputs)\n",
    "    outputs = (outputs * 0.5) + 0.5\n",
    "    cnnlstm_model = models.Model(inputs, outputs, name='xception_cnnlstm')\n",
    "    cnnlstm_model.compile(optimizer = Adam(1e-3), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    callbacks = get_callbacks(modelname, es_patience = 6, rlr_patience = 4, verbose = 1)\n",
    "    history = cnnlstm_model.fit(train_datagen, verbose = 2, steps_per_epoch = train_spe, epochs = epochs//2, validation_data = val_datagen, validation_steps = val_spe, callbacks = callbacks)\n",
    "    best_valloss = callbacks[0].best\n",
    "    cnnlstm_model = load_model(f'{modelname}.h5')\n",
    "    test_datagen = video_data_generator(dataset,'test', batch_size = batch_size)\n",
    "    print(cnnlstm_model.evaluate(test_datagen,steps=2*test_spe,verbose=0))\n",
    "\n",
    "    base_model.trainable = True\n",
    "    cnnlstm_model.compile(optimizer = Adam(1e-6), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    callbacks = get_callbacks(modelname + '_finetuned', es_patience = 7, rlr_patience = 4, verbose = 1)\n",
    "    callbacks[0].best = best_valloss\n",
    "    history = cnnlstm_model.fit(train_datagen, verbose = 2, steps_per_epoch = train_spe, epochs = epochs//2, validation_data = val_datagen, validation_steps = val_spe, callbacks = callbacks)\n",
    "    cnnlstm_model = load_model(f'{modelname}.h5')\n",
    "    test_datagen = video_data_generator(dataset,'test', batch_size = batch_size)\n",
    "    print(cnnlstm_model.evaluate(test_datagen,steps=2*test_spe,verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image\n",
    "\n",
    "model_path = f'models/CNN' # Save models here\n",
    "Path(model_path).mkdir(parents=True, exist_ok = True)\n",
    "\n",
    "epochs = 40\n",
    "batch_size = 16\n",
    "\n",
    "train_spe = len(os.listdir(f'{dataset}/train/real'))//batch_size\n",
    "print(train_spe)\n",
    "val_spe = len(os.listdir(f'{dataset}/val/real'))//batch_size\n",
    "print(val_spe)\n",
    "test_spe = len(os.listdir(f'{dataset}/test/real'))//(batch_size//4)\n",
    "print(test_spe)\n",
    "\n",
    "# Train multiple times\n",
    "for attempt in range(3):\n",
    "    print(f'Attempt #{attempt}')\n",
    "    foldername = f'{model_path}/{dataset}_xceptionimagenet_attempt{attempt}'\n",
    "    Path(foldername).mkdir(parents=True, exist_ok=True)\n",
    "    modelname = f'{foldername}/{dataset}_xceptionimagenet_{num_images}_{img_size}'\n",
    "\n",
    "    train_datagen = video_data_generator(dataset,'train', batch_size = batch_size)\n",
    "    val_datagen = video_data_generator(dataset,'val', batch_size = batch_size)\n",
    "    test_datagen = video_data_generator(dataset,'test', batch_size = batch_size)\n",
    "\n",
    "    base_model = Xception()\n",
    "    base_model.load_weights('imagenet')\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    inputs = layers.Input(shape=(None,None,None,3))\n",
    "    outputs = base_model(inputs, training=False)\n",
    "    outputs = layers.TimeDistributed(layers.Dropout(0.5), name = 'globavgpool_dropout')(outputs)\n",
    "    outputs = layers.TimeDistributed(layers.Dense(1,'sigmoid'),name='predictions')(outputs)\n",
    "    outputs = layers.GlobalAveragePooling1D(dtype='float32',name='average')(outputs)\n",
    "    cnn_model = models.Model(inputs, outputs, name='xception_cnn')\n",
    "    cnn_model.compile(optimizer = Adam(1e-3), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    callbacks = get_callbacks(modelname, es_patience = 6, rlr_patience = 4, verbose = 1)\n",
    "    history = cnn_model.fit(train_datagen, verbose = 2, steps_per_epoch = train_spe, epochs = epochs//2, validation_data = val_datagen, validation_steps = val_spe, callbacks = callbacks)\n",
    "    best_valloss = callbacks[0].best\n",
    "    cnn_model = load_model(f'{modelname}.h5')\n",
    "    test_datagen = video_data_generator(dataset,'test', batch_size = batch_size)\n",
    "    print(cnn_model.evaluate(test_datagen,steps=2*test_spe,verbose=0))\n",
    "\n",
    "    base_model.trainable = True\n",
    "    cnn_model.compile(optimizer = Adam(1e-6), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    callbacks = get_callbacks(modelname + '_finetuned', es_patience = 7, rlr_patience = 4, verbose = 1)\n",
    "    callbacks[0].best = best_valloss\n",
    "    history = cnn_model.fit(train_datagen, verbose = 2, steps_per_epoch = train_spe, epochs = epochs//2, validation_data = val_datagen, validation_steps = val_spe, callbacks = callbacks)\n",
    "    cnn_model = load_model(f'{modelname}.h5')\n",
    "    test_datagen = video_data_generator(dataset,'test', batch_size = batch_size)\n",
    "    print(cnn_model.evaluate(test_datagen,steps=2*test_spe,verbose=0))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "td-cnn-lstm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
